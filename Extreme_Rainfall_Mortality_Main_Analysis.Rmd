---
title: "Extreme Rainfall and Mortality"
author: "Kevin Chan"
date: "12/10/2023"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo   =FALSE,      ## show or suppress the code
                      include=TRUE ,      ## show or suppress the output
                      message=FALSE,      ## omit messages generated by code
                      warning=FALSE,      ## omit warnings generated by code
                      comment=NA,         ## removes the ## from in front of outputs
                      fig.align="center", ## centers all figures
                      fig.height = 5,     ## set the default height
                      fig.weight = 5      ## set the default width
                      )
```

## Abstract
Extreme rainfall events have been shown to be a risk factor for human health, but the associations between extreme rainfall events and mortality are rarely explored. Consequently, there is a knowledge gap pertaining to whether or not extreme rainfall events cause an uptick in mortality and if so, by how much and when. For our project, we conducted a time-series study to answer these questions by using county-level cause-specific North Carolina mortality data on the years 2015-2018 obtained from the North Carolina Department of Health and Human Services. The mortality data itself is split into total mortality, non-accidental mortality, respiratory disease mortality, cardiovascular disease mortality, and external mortality. We paired the mortality data with rainfall data from the National Oceanic and Atmospheric Administration to measure exposure. We then adopted a two-stage analysis where we first used a quasi-Poisson generated linear regression model to estimate the associations for each individual county while lagging the exposures to extreme rainfall to understand the time scale. We also controlled for daily temperature data from ERA5-Land, the date, and the day of week in our model. Once we obtained model estimates for extreme rainfall exposure within each county for several lag days, we performed a meta-analysis to obtain a state-wide association. The model gave us relative risk estimates along with confidence intervals, so lag days where the lower bounds were greater than 1.00 indicate statistical significance. The relative risk confidence intervals not only give us insight into the time-structure of the exposure, but also on the magnitude itself since, for example, a relative risk of 1.05% on lag day 2 would mean that we see a 5% increase in mortality 2 days after exposure. In our analysis, we found that exposure to an extreme rainfall event is associated with an increase in total, non-accidental, cardiovascular disease, respiratory disease, and external mortality by by 2.24% (95% CI: 0.67%, 3.84%), 2.37% (95% CI: 0.67%, 4.02%), 3.58% (95% CI: 0.67%, 6.57%), 6.67% (95% CI: 1.66%, 11.93%), and 6.92% (95% CI: 1.27%, 12.88%), respectively. We also conducted a subgroup analysis with different sex and age groups, but did not find significant differences in the mortality risks among them. To ensure that our model was valid, we performed various sensitivity analyses such as varying the degrees of freedom for our variables and discovered that our results remained robust.

## Introduction 
Heavy precipitation over land is directly related to the increasing global warming and to climate-change-driven tropical cyclones, which have increased in frequency and overall intensity since the 1950s. The Intergovernmental Panel on Climate Change (IPCC) Sixth Assessment Report defines extreme precipitation as the annual maximum daily precipitation being exceeded on average once during a 10-year period between the years 1850-1900. The report further states that extreme precipitation events could intensify by about 7% with each additional 1°C of warming. In the warmer future with our current and pledged policies, land precipitation is projected to increase by 1.5-8% globally under the intermediate emissions scenario (SSP2-4.5), which would likely strengthen the intensity and frequency of extreme rainfall events in North America. As a notable example, global warming increased the rainfall intensity during Hurricane Harvey by 15% (uncertainty interval of 8% and 19%) in 2017 when it made landfall in the United States. The tropical storm resulted in the loss of $125 billion (confidence interval from $90 to $160 billion) for the US. Thus, it is imperative that we understand the health impacts of extreme rainfall events in order to support public health responses to these dangerous weather events.	Extreme rainfall has been shown to cause negative health impacts with previous studies focusing on the morbidity of communicable diseases. Researchers have observed positive associations between short-term heavy rainfall exposure and increased hospitalization risks for waterborne diseases along with the rate of emergency room visits for gastrointestinal illnesses. It is also worth noting that several studies have found that extreme rainfall could be associated with symptoms of non-communicable disease such as airway inflammation in adolescents and outpatient visits for depression. Additionally, one study has found an association between a 10-millimeter increase in exposure to daily precipitation and all-cause mortality. However, a knowledge gap still exists due to the previous studies’ focus on morbidity or solely all-cause mortality, resulting in limited evidence on the relationship with cause-specific mortality. Evidence examining the effects of extreme rainfall events on cause-specific mortality using a large study population over a multi-year time scale is needed.

We will now outline the rest of this report. In Section 2, we show why our data lends itself to a time series model through data exploration and visualization. In Section 3, we test generalized linear models (glm) with different variables before arriving at our baseline model, a glm with the 95th percentile benchmark for extreme rainfall exposure. We then discuss the results of the model in Section 4 and whether or not similar studies on extreme rainfall have yielded similar results. We finish by discussing conclusions, recommendations, and ideas for future work in the Section 5.


## Data Exploration and Visualization
The daily all-cause and cause-specific mortality data within each county in North Carolina between the years 2015-2018 were obtained from the North Carolina Department of Health and Human Services, more specifically the Vital Statistics Department. We obtained data on the total mortality and four cause-specific deaths categorized based on the International Classification of Diseases, 10th version (ICD-10) code, including non-accidental deaths, cardiovascular disease deaths, respiratory disease deaths, and external deaths. We note that total death is equivalent to the combined number of non-accidental and external deaths. When we refer to external deaths, we refer to death from injuries or suicide. While data was missing in the mortalities, we could not code them in our data frame as simply 0 because mortalities are the response variable for our model and having additional zeros would bias the model towards 0. Thus, we were forced to leave the days without a mortality report for certain counties out. The daily county-level meteorological variables are daily total rainfall and daily average temperature. The daily total rainfall was obtained from the National Oceanic and Atmospheric Administration with a spatial resolution of 0.25-degree latitude by 0.25-degree longitude. The daily average temperature was obtained from the ERA5-Land hourly data from 1950 to the present with a spatial resolution of 0.1-degree latitude by 0.1-degree longitude. The county-level daily total rainfall and mean temperature were calculated as the area-weighted average of raster values that intersect the respective county. The data sets were complete without any missing information. We identify each county based on their FIPS codes, which are numbers that uniquely identify each county in the United States. We also obtained population data from the US Census Bureau but the data was on the scale of years. We ended up not requiring population in our models. Now let us load the data and see the summary tables for mortality by county per year.

We consistently see county 37183 or Wake County with the highest mortality rate across the entire 2015-2018 time period. We also create a histogram by taking the daily rainfall data of all counties and we can see that the distribution is right-skewed, implying that rainfall in North Carolina is consistently less than 20 millimeters. We also see that the majority of days in the data set experience no rainfall at all. We first print the following summary statistics and a histogram for the precipitation measurements:

```{r}
#We import the libraries
library(dplyr)
library(tidyr)
library(reshape)
library(lubridate)
library(ggthemes)
library(ggplot2)
library(ggstance)
library(stringr)
library(tsModel)
library(splines)
library(metafor)
library(splines)

library(gnm)
library(AER)
library(cowplot)
```

```{r}
#Loading and cleaning the data
#Note that I have already merged the majority of the data to reduce the size of the Rmd
dailyCT<-read.csv("NCExposureData.csv",header = T)%>% 
  dplyr::select(-X,-caseControl)
dailyCT$date<-as.Date(dailyCT$date)

#Add the 7th lag
dailyCT$prcp95th_lag7 <- tsModel::Lag(dailyCT$prcp95th_lag0, 7, group = as.factor(dailyCT$FIPS))

#Now we load the cause specific mortality data
mdata <- read.csv("CauseSpecificMortality.csv", header = TRUE)
mdata$date <- as.Date(mdata$date)

#Now we create the temperature lags within each county 
regions <- as.character(unique(dailyCT$FIPS))
dlist <- lapply(regions,function(x) dailyCT[dailyCT$FIPS==x,])
names(dlist) <- regions
for(i in seq(length(dlist))) {
  #Print to keep track of the county we are on
  cat(i,"")
  
  #Extract the data
  data <- dlist[[i]]
  
  #Lags
  data$templag0<-data$temp
  data$templag1<-Lag(data$temp,1)
  data$templag2<-Lag(data$temp,2)
  data$templag3<-Lag(data$temp,3)
  data$templag4<-Lag(data$temp,4)
  data$templag5<-Lag(data$temp,5)
  data$templag6<-Lag(data$temp,6)
  data$templag7<-Lag(data$temp,7)
  
  #time
  data$time<-c(1:length(data$date))
  
  if(i==1){
    lagresult<-data
  }else{
      lagresult<-rbind(lagresult,data)
  }
}

lagresult<-dplyr::select(lagresult,-totdeath)
alldata<-dplyr::left_join(mdata,lagresult,by=c("FIPS","date"))

#Because we want to get rid of the index column
mydata <- alldata[,-1]
```

```{r}
#Get summary statistics for the mortalities by county but ordered
#We want to get the mortality rate for each county per year
fips <- unique(mydata$FIPS)
mortRate15 <- data.frame(fips, pop15 = NA, mortSum = NA, Ratio15 = NA)
mortRate16 <- data.frame(fips, pop16 = NA, mortSum = NA, Ratio16 = NA)
mortRate17 <- data.frame(fips, pop17 = NA, mortSum = NA, Ratio17 = NA)
mortRate18 <- data.frame(fips, pop18 = NA, mortSum = NA, Ratio18 = NA)

for(i in 1:length(fips)){
  temp <- mydata[mydata$FIPS == fips[i], ]
  
  temp15 <- temp[temp$year == 2015, ]
  mortRate15$pop15[i] <- temp15[1, "pop"]
  mortRate15$mortSum[i] <- sum(temp15$totdeath)
  mortRate15$Ratio15[i] <- mortRate15$mortSum[i]/mortRate15$pop15[i]
  
  temp16 <- temp[temp$year == 2016, ]
  mortRate16$pop16[i] <- temp16[1, "pop"]
  mortRate16$mortSum[i] <- sum(temp16$totdeath)
  mortRate16$Ratio16[i] <- mortRate16$mortSum[i]/mortRate16$pop16[i]
  
  temp17 <- temp[temp$year == 2017, ]
  mortRate17$pop17[i] <- temp17[1, "pop"]
  mortRate17$mortSum[i] <- sum(temp17$totdeath)
  mortRate17$Ratio17[i] <- mortRate17$mortSum[i]/mortRate17$pop17[i]
  
  temp18 <- temp[temp$year == 2018, ]
  mortRate18$pop18[i] <- temp[1, "pop"]
  mortRate18$mortSum[i] <- sum(temp18$totdeath)
  mortRate18$Ratio18[i] <- mortRate18$mortSum[i]/mortRate18$pop18[i]
}

#mortRate15[order(mortRate15$Ratio15), ]
#mortRate16[order(mortRate16$Ratio16), ]
#mortRate17[order(mortRate17$Ratio17), ]
#mortRate18[order(mortRate18$Ratio18), ]
```

We first print the following summary statistics:
```{r}
#Now get the number of extreme precipitation days
fips <- unique(mydata$FIPS)

#Vector to hold the number of precipitation days
prcpNum <- c()

#Vector to hold the number of all-cause mortality
allMortNum <- c()

#Vector to hold the number of non-accidental mortality
nonaccMortNum <- c()

#Vector to hold the number of cardiovascular mortality
cvdMortNum <- c()

#Vector to hold the number of respiratory mortality
rsdMortNum <- c()

#Vector to hold the number of external mortality
extMortNum <- c()

#Vector to hold the average temperature
tempNum <- c()

#Loop to get the values of interest
for(i in 1:length(fips)){
  #Summation for the number of precipitation days and mortality types
  prcpNum[i] <- sum(mydata[mydata$FIPS == fips[i], "prcp_95th"])
  allMortNum[i] <- sum(mydata[mydata$FIPS == fips[i], "totdeath"])
  nonaccMortNum[i] <- sum(mydata[mydata$FIPS == fips[i], "death_nonacc"])
  cvdMortNum[i] <- sum(mydata[mydata$FIPS == fips[i], "death_CVD"])
  rsdMortNum[i] <- sum(mydata[mydata$FIPS == fips[i], "death_RSD"])
  extMortNum[i] <- sum(mydata[mydata$FIPS == fips[i], "death_external"])
  
  #Average each county's data for the temperature
  tempNum[i] <- mean(mydata[mydata$FIPS == fips[i], "temp"])
}

print("Summary of Extreme Precipitation Occurrences in Counties")
summary(prcpNum)

print("Summary of All Cause Mortality in Counties")
summary(allMortNum)

print("Summary of Non-Accidental Mortality in Counties")
summary(nonaccMortNum)

print("Summary of Cardiovascular Mortality in Counties")
summary(cvdMortNum)

print("Summary of Respiratory Mortality in Counties")
summary(rsdMortNum)

print("Summary of External Mortality in Counties")
summary(extMortNum)

print("Summary of Average Temperature in Counties")
summary(tempNum)
```

```{r}
library(pubtheme)
hist_rain <- ggplot(mydata, aes(x = prcp)) + 
  geom_histogram(fill = pubblue, color = pubbackgray, binwidth = 2) + 
  labs(x = "Precipitation in Millimeters",
       y = "Histogram Count")
  
hist_rain
```

Fig. 1: Histogram of the Amount of Precipitation Experienced By All Counties in Millimeters  

We consistently see county 37183 or Wake County with the highest mortality rate across the entire 2015-2018 time period. We also create a histogram by taking the daily rainfall data of all counties and we can see that the distribution is right-skewed, implying that rainfall in North Carolina is consistently less than 20 millimeters. We also see that the majority of days in the data set experience no rainfall at all.

In this study, an extreme rainfall event is defined as when the daily precipitation amount exceeds a certain threshold, a definition similar to the IPCC Sixth Assessment Report. While the selection of threshold varies, most literature use the percentile method to define the threshold. Some common percentiles are the 90th, 95th, and 99th. In our study, we chose to use the 95th percentile of the daily precipitation during 2015-2018 across all 100 counties. The baseline of 95th was chosen because using the 99th percentile yielded a low number of extreme rainfall days and there weren’t any interesting patterns when using the 90th. We then defined an extreme rainfall exposure based on whether the specific county experienced rainfall that day greater than or equal to the 95th percentile threshold (20.44 mm). Days that experienced no precipitation or precipitation less than the 95th percentile were classified as non-extreme rainfall days. We plot the number of extreme rainfall days per county with the following: 
```{r}
extPrcpDay <- cbind(fips, prcpNum)

#Turn into a data frame
extPrcpDay <- as.data.frame(extPrcpDay, row.names = c("FIPS", "ExtPrcpDays"))

library(tigris)
options(tigris_use_cache = TRUE)

#Subset to get the NC polygon data
censusdata <- tracts(state = "NC")

#Paste Together to create a column that's mergeable
censusdata$CountyFIP <- paste0(censusdata$STATEFP, censusdata$COUNTYFP)
censusdata$CountyFIP <- as.integer(censusdata$CountyFIP)

#Merge the data frames
spaceMapData <- geo_join(censusdata, extPrcpDay, by_sp = "CountyFIP", by_df = "fips")

#Only get the land, gets rid of one whole county
spaceMapData <- subset(spaceMapData, ALAND > 0)

#Now we map with ggplot
#color = "#D5DBDB"
prcpCountyMap <- ggplot(spaceMapData) + 
  geom_sf(aes(fill = prcpNum), size = 0.05, color = NA) + 
  scale_fill_gradient2() +
  theme_void() +
  labs(fill = "Number of Days") + 
  theme(plot.title = element_text(hjust = 0.5), legend.position = c(0.3, 0.15))

library(ggspatial)
#Add a compass
#Add the length ratio
finalprcpMap <- prcpCountyMap +
  ggspatial::annotation_scale(
    location = "tr",
    pad_y = unit(2.67, "in"),
    bar_cols = c("grey60", "white")
  ) +
  ggspatial::annotation_north_arrow(
    location = "tr", which_north = "true",
    pad_x = unit(0.9, "in"), pad_y = unit(2, "in"),
    style = ggspatial::north_arrow_nautical(
      fill = c("grey40", "white"),
      line_col = "grey20",
    )
  )

finalprcpMap
```
Fig 2: Map of the Number of Extreme Rainfall Days for each County, 2015-2018

And evidently, we see a higher concentration of extreme precipitation days by the coast. We also see that with the exception of the lower right-hand side, there seems to be more of an occurrence in the middle of the state than on the outer edges. We also note the absence of the counties Avery, Hyde, and Tyrell which were purposely excluded due to their models’ inability to converge because of their small sample sizes for cause-specific mortalities like respiratory disease mortality. For the sake of consistency, we left the three counties out for every model. If we had not, then our models would constantly be throwing “model cannot converge” errors.

We can also plot the distribution of extreme rainfall days by month for North Carolina and within each county. We can add color to the bar plot to understand the distribution across the four different years. We plot for the counties: 
```{r}
#We want to plot a distribution of the number of extreme data points by month and also distinguish between the years
#Let's create a data frame that makes this easier for us
#First we get all of the instances of extreme precipitation
extDistData <- mydata[mydata$prcp_95th == 1, ]

#Add the month column
extDistData$month <- format(as.Date(extDistData$date, format = "%Y-%m-%d"), "%m")

#Next we get how many extreme precipitation instances occurred within each day
extDistData <- extDistData[, c("date", "year", "month", "prcp_95th")]

#Create a data frame that contains all of the dates, their year, and how many extreme precipitation exposures occurred
extData <- extDistData %>%
  group_by(month, year) %>%
  summarize(Days = n())

extDistPlot <- ggplot(data = extData, aes(x = month, y = Days, na.rm = TRUE, fill = forcats::fct_rev(factor(year, ordered = FALSE)))) + 
  geom_bar(stat = "identity", width = 0.7) + 
  guides(fill=guide_legend(title = "Year")) + 
  xlab("Month") +
  ylab("Total Number of Extreme Rainfall Exposures (Counties)") +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(), axis.line = element_line(colour = "black"))
extDistPlot
```
Fig 3: Bar Plot of the Total Number of Extreme Rainfall Exposures within Counties Across Month Subsetted by Year

And we can see that the spring and the fall seasons are when we have the most amount of county exposures to extreme rainfall during our 2015-2018 time period. We now plot for the entire state:
```{r}
rainDates <- data.frame(date = unique(extDistData$date))

#Now we get the month and year 
rainDates$month <- format(rainDates$date, "%m")
rainDates$year <- format(rainDates$date, "%Y")

#Adding another column of 1s
rainDates$count <- 1

#Plot the graph
rainByDayPlot <- ggplot(data = rainDates, aes(x = month, y = count, na.rm = TRUE, fill = forcats::fct_rev(factor(year, ordered = FALSE)))) + 
  geom_bar(stat = "identity", width = 0.7) + 
  guides(fill=guide_legend(title = "Year")) + 
  xlab("Month") +
  ylab("Total Number of Extreme Rainfall Days") +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(), axis.line = element_line(colour = "black"))
rainByDayPlot
```
Fig 4: Bar Plot of the Total Number of Extreme Rainfall Exposures for the State of North Carolina Across Month Subsetted by Year

The bar plot shows that state-wide, we have more extreme rainfall days in the summer across the time frame. When we look at summary statistics in table 1 of the appendix, we see that there are a total of 5,302 extreme rainfall days experienced by the North Carolina counties during the 2015-2018 time span. We can now also plot the number of deaths per day by taking the cumulative sum of all the deaths within each county:
```{r} 
#Aggregate the data
cumprep <- data.frame(date = unique(mydata$date), prep = NA, mort = NA)

for(i in 1:length(cumprep$date)){
  cumprep$prep[i] <- sum(mydata$prcp[mydata$date == cumprep$date[i]])
  cumprep$mort[i] <- sum(mydata$totdeath[mydata$date == cumprep$date[i]])
}
#Run some tests on the aggregated data
plot(cumprep$date, cumprep$mort, xlab = "Date", ylab = "Daily Mortality", pch = 16)
```
Fig. 5: Time-Series Scatterplot of the Daily Mortality in North Carolina vs. the Date


## Modeling/Analysis
We adopted a two-stage time-series study design. In the first stage, we estimated the association between daily cause-specific deaths and exposure to extreme rainfall events using the quasi-Poisson generalized linear regression model for each county. Since the variances of the total and cause-specific mortalities are larger than their corresponding means, quasi-Poisson is appropriate to account for the overdispersion. We note the low correlation coefficient of 0.023 between extreme rainfall days and temperature, indicating collinearity to be unlikely and that we should control for temperature in our model.  We used the following formula for our base model:
![model](ModelParamCapstone.png)
where t is the day of observation; μt denotes the daily count of deaths on the given day t, is the intercept of the model, EP is the indicator for an extreme precipitation exposure on day t, is the coefficient for EP, temperature is the daily mean temperature, date is the date of the day, DOW is the day of week, and γ is the coefficient for DOW. ns represents taking the natural cubic spline of the variables. For our analysis, we lag the extreme rainfall exposure by days 0-7 in order to determine the time trend of the relationship. Thus, the base model is actually a set of eight models instead of one model. We also tried modeling with generalized nonlinear models (gnm) and generalized additive models (gam), but one of the assumptions of the gnm is that there is a specified functional form relating the linear predictor to the expected value of the response variable. While the gnm does allow for nonlinearities, for the purposes of our study, we will do just fine with glms. Glms operate under the assumption that the relationship between the linear predictor and the expected value of the response variable is linear. By nature, this makes the glm easily interpretable. We also have low multicollinearity, adhering to another one of glm requirements. 

We used a natural cubic spline with 5 degrees of freedom (df) to estimate the delayed effects of temperature and a natural cubic spline with 6 degrees of freedom per year for the date as these are standard values in the literature. Natural cubic splines are piece-wise cubic polynomials that are twice differentiable and they allow for us to almost “bend” the function we will be fitting to the data. As shown in figure 5 above, our model needs to be in the shape of a polynomial and natural cubic splines help us do so.

We examined the single day lags 0-7 of EP on overall mortalities to capture the lag pattern. We also matched the daily temperature of the same lag day with EP in the model. We further conducted a subgroup analysis by sex (male and female), age (0-64, 65-74, 75+ years old), race (non-Hispanic black and non-Hispanic white), and seasonality (May to September for the warm season and October to April for the cold season). We note that we are not aiming for a predictive model for the future, but instead would like to fit our time-series model to only the data we have. Thus, we want our fit to be as accurate as possible. Additionally, for our purposes, we are interested in β since we want to know the effect the variable EP has on mortality, which is what the model has as its response variable. Thus, with the value from each run of the model, we can take e to the value of β to get the exposure-response relationship since the y-variable is logarithmic with base e. 

In the second stage, we performed a meta-analysis to obtain the pooled exposure-response relationships. We used a meta-analysis model based on the restricted maximum likelihood estimator to predict the coefficients and to calculate the 95% confidence intervals (95% CI) of the relative risks. We exclude two counties for external and one county for respiratory deaths because the model was unable to converge. 

We tested the robustness of our results with four different sensitivity analyses: 1) varying the degrees of freedom for mean temperature (df = 4 and df = 6) and for the date (df = 5 per year and df = 7 per year) separately; 2) we tried using the 90th and 97.5th percentiles as thresholds to define an extreme rainfall event; 3) we adjusted the main model by replacing the same-day lag temperature with a three-day moving average temperature; 4) we additionally controlled for holiday. To assess heterogeneity between the categories in each subgroup, we further conducted two sample z-tests and found no significance between the estimates. Our calculation is as follows:
![ztest](z-scoreCapstone.png)

where  beta_hat_1 and beta_hat_2 are the regression coefficients for the two categories, and SE_hat_1 and SE_hat_2 are their respective standard errors. For our analysis, we used the “metafor” package in R software (version 4.1.3). 

One assumption for the analysis is that temperature follows a seasonal trend, hence why we place the variable inside a natural spline. There are trends within the date as well, hence why we also used a natural spline. These are common, well-established methods of treating temperature and the date within the field of environmental epidemiology. Adherence to the common methods is why we also keep the day of week and the exposure as linear variables. Quasi-Poisson time series models are widely used as well and work well for our data as we want to see the changes in mortality over time across different counties, but have scarce instances of extreme rainfalls and high variance. (Appendix Table 1).



## Visualization and Results
We ran the base model and the meta-analysis to obtain the following:

```{r}
#Now we run our models using the different mortality types. We define the mortalities and the exposure lags
vars <- c("totdeath", "death_nonacc", "death_CVD", "death_RSD", "death_external")
lags <-  c("prcp95th_lag0","prcp95th_lag1","prcp95th_lag2","prcp95th_lag3","prcp95th_lag4","prcp95th_lag5","prcp95th_lag6", "prcp95th_lag7")

#Now define the regions and separate the FIPs in the general data frame into lists
regions <- as.character(unique(mydata$FIPS))
dlist <- lapply(regions,function(x) mydata[mydata$FIPS==x,])
names(dlist) <- regions

#Loop through the different mortalities
mod_results <- data.frame()
for(death_type in vars){
  #Define a data frame to hold the model results
  metaresults <- data.frame(cat=lags, fit = NA, se = NA)
  
  #For loop to loop through the FIPs
  for(i in seq(length(dlist))){
    #Print the FIPs as we run through the for loop
    cat(i, "")
    
    #Extract the data
    subdata <- dlist[[i]]
    
    #Now define the for loop through the lag days for the actual model
    for(j in 0:7){
      #To get around the algorithm not converging for some counties
      if(death_type == "totdeath"){
        mod1 <- glm(subdata[, death_type] ~ as.factor(subdata[, paste0("prcp95th_lag", j)]) 
                    + ns(time, 4*6) + ns(subdata[, paste0("templag", j)], 5) + dow,
                    data = subdata, family = quasipoisson)
      }else{
        trynext = try(
        mod1 <- glm(subdata[, death_type] ~ as.factor(subdata[, paste0("prcp95th_lag", j)]) 
                    + ns(time, 4*6) + ns(subdata[, paste0("templag", j)], 5) + dow,
                    data = subdata, family = quasipoisson), silent = F)
        #To get around the algorithm not converging for some counties
        if("try-error" %in% class(trynext)){
          next
        }
      
        mod1 <- glm(subdata[, death_type] ~ as.factor(subdata[, paste0("prcp95th_lag", j)]) 
                      + ns(time, 4*6) + ns(subdata[, paste0("templag", j)], 5) + dow,
                      data = subdata, family = quasipoisson)
      }
      metaresults[j+1, 2:3] <- c(summary(mod1)$coef[2,1], summary(mod1)$coef[2,2])
      metaresults$dis[j+1] <- death_type
      metaresults$county[j+1] <- unique(subdata$FIPS)
    }
    #Create a data frame to hold the results
    if(i == 1){
      results <- metaresults
    }else{
      results <- rbind(results, metaresults)
    }
  }
  mod_results <- rbind(mod_results, results)
}

output_results <- lapply(vars, function(x) mod_results[mod_results$dis == x, ])
```

```{r}
temp <- output_results

#Now we conduct the meta-analysis
#Get rid of the NAs and redefine the lags, vars, along with a new metaresults dataframe
countyresult<-na.omit(temp)
vars <- c("totdeath", "death_nonacc", "death_CVD", "death_RSD", "death_external")
lags <-  c("prcp95th_lag0","prcp95th_lag1","prcp95th_lag2","prcp95th_lag3","prcp95th_lag4","prcp95th_lag5","prcp95th_lag6", "prcp95th_lag7")


for(i in 1:length(vars)){
  #First subset the lists to the correct death type
  cat(vars[i], "")
  data <- countyresult[[i]]
  
  #Now define a data frame to hold the model results for each lag type
  metaresults <- data.frame(cat=lags, fit = NA, low = NA, upper = NA)
  
  #Now we can run the meta-analysis model
  for(j in seq(length(lags))){
    #Print the lag we are on
    cat(j, "")
    #Subset the data further on the lag type
    prcp_lag <- subset(data, data$cat == lags[j])
    
    fit_est <- prcp_lag$fit
    fit_se <- prcp_lag$se
    
    #Now for the meta-analysis model
    metamod <- rma(yi = fit_est, sei = fit_se, data = prcp_lag, method = "REML")
    metaresults[j, 2] <- predict(metamod, transf = exp, digits = 4)$pred
    metaresults[j, 3] <- predict(metamod, transf = exp, digits = 4)$ci.lb
    metaresults[j, 4] <- predict(metamod, transf = exp, digits = 4)$ci.ub
    
    metaresults$dis[1:nrow(metaresults)] <- vars[i]
    metaresults$thres[1:nrow(metaresults)] <- "95th"
  }
  
  #Now we create a place to store the results for each mortality type
  if(i == 1){
    allmetaresult <- metaresults
  }else{
    allmetaresult <- rbind(allmetaresult, metaresults)
  }
}

holder <- allmetaresult
holder$Analysis <- "BasicAnalysis"
```
![Fig 6.](Figure3.png)
Fig 6: Base Model Confidence Intervals for Each Lag Day. RR = Relative Risk, CVD = Cardiovascular Disease Mortality, RSD = Respiratory Disease Mortality.

Figure 6 displays the relative risks and their corresponding confidence intervals for all causes of mortality across all lag days. Our results indicated that exposure to extreme rainfall events is significantly associated with total mortality on lag day 3 and lag day 5 (RR = 1.0202, 95% CI: 1.0069, 1.0336) separately. No significant association was observed for the other lag days. The non-accidental and respiratory mortalities displayed similar lag exposure patterns with the highest estimates on lag day 3. Cardiovascular and external mortality displayed the highest relative risks on lag days 0 and 1 respectively.

![Image.](Figure2.png)
Fig. 7: Significant Confidence Intervals for the Base Model

Figure 7 presents the highest estimates of the relative risks and the corresponding confidence intervals for all-cause and cause-specific mortality. For exposure to extreme rainfall events, the relative risk for total mortality is 1.0224 (95% CI: 1.0067, 1.0384). The relative risks for non-accidental mortality, cardiovascular disease mortality and respiratory disease mortality are 1.0237 (95% CI: 1.0067, 1.0402), 1.0358 (95% CI: 1.0067, 1.0657) and 1.0667 (95% CI: 1.0166, 1.1193), respectively. External mortality is also significantly associated with extreme rainfall events with a relative risk of 1.0692 (95% CI: 1.0127, 1.1288). The lag days give us perspective on how many days after the initial exposure do we see the corresponding relative risk confidence interval. As an example, since total mortality at lag 3 had a result of 1.0224 (95% CI: 1.0067, 1.0384), this means that 3 days after exposure to extreme rainfall, we see an increase of 2.24% in total mortality in the state of North Carolina. Thus, our confidence intervals and lag days quantify the impact of extreme rainfall on mortality and quantify the time scale. Additionally, note that in figure 6, lag day 5 for total mortality, non-accidental, and respiratory disease are also above the 1.00 threshold, but have a lower upper bound on its confidence interval when compared to the intervals of lag day 3. Since we are treating each lag separately within its own model, we can explain the result as an extension of lag day 3’s significance but this does not however, explain why lag day 4 remains insignificant. Consequently, more research must be done.

After running our base model, we performed the following sensitivity analyses:
1) Using the 90th percentile to define an extreme rainfall day
2) Using the 97.5th percentile to define an extreme rainfall day
3) Setting the degrees of freedom for temperature equal to 4 per year under the 95th percentile definition
4) Setting the degrees of freedom for temperature equal to 6 per year under the 95th percentile
5) Using a three-day moving average of the temperature in the main model using the 95th percentile definition
6) Using 5 degrees of freedom per year in the natural cubic spline for date under the 95th percentile definition
7) Using 7 degrees of freedom per year in the natural cubic spline for date under the 95th percentile definition
8) Controlling for holiday

We opted to define our base model as the basis since the use of the 95th percentile is common in environmental epidemiology literature linking exposure and health outcomes. Another motivator was when we saw a lack of significance in our 90th percentile definition. Across the board, it seems like our model remained robust against the sensitivity analyses and preserved similar lag structure and significance. Please refer to the appendix for a table with the full sensitivity analysis results. We also note that aside from the shift in percentiles, the interpretability of our model doesn't change significantly since we can always obtain the relative risk by cancelling out the logarithm. 

We performed a subgroup analysis for total mortality on lag day 3.
```{r}
#Loading and cleaning the data
dailyCT<-read.csv("NCExposureData.csv",header = T)%>% 
  dplyr::select(-X,-caseControl)
dailyCT$date<-as.Date(dailyCT$date)

#Add the 7th lag
dailyCT$prcp95th_lag7 <- tsModel::Lag(dailyCT$prcp95th_lag0, 7, group = as.factor(dailyCT$FIPS))

#Now we load the cause specific mortality data
mdata <- read.csv("CauseSpecificMortality.csv", header = TRUE)
mdata$date <- as.Date(mdata$date)

#Now we create the temperature lags within each county 
regions <- as.character(unique(dailyCT$FIPS))
dlist <- lapply(regions,function(x) dailyCT[dailyCT$FIPS==x,])
names(dlist) <- regions
for(i in seq(length(dlist))) {
  #Print to keep track of the county we are on
  cat(i,"")
  
  #Extract the data
  data <- dlist[[i]]
  
  #Lags
  data$templag0<-data$temp
  data$templag1<-Lag(data$temp,1)
  data$templag2<-Lag(data$temp,2)
  data$templag3<-Lag(data$temp,3)
  data$templag4<-Lag(data$temp,4)
  data$templag5<-Lag(data$temp,5)
  data$templag6<-Lag(data$temp,6)
  data$templag7<-Lag(data$temp,7)
  
  #time
  data$time<-c(1:length(data$date))
  
  if(i==1){
    lagresult<-data
  }else{
      lagresult<-rbind(lagresult,data)
  }
}


lagresult<-dplyr::select(lagresult,-totdeath)
alldata<-dplyr::left_join(mdata,lagresult,by=c("FIPS","date"))

#Because we want to get rid of the index column
mydata <- alldata[,-1]

#Load in the subgroup data
subgroupData <- read.csv("NCsubgroups.csv", header = T)

#Convert the dates into a data variable type
subgroupData$date <- as.Date(subgroupData$date, "%Y-%m-%d")
#Getting rid of the first column
subgroupData <- subgroupData[, -1]

#Now merge the data frames together
mydata <- left_join(mydata, subgroupData, by = c("FIPS", "date"))

#Now let's get the columns we need for what we will run the analysis on 
subgroups <- c("totdeath_f", "totdeath_m", "totdeath_064", "totdeath_6574", "totdeath_75")

#Now we run our models using the different mortality types. We define the mortalities and the exposure lags
lags <-  c("prcp95th_lag0","prcp95th_lag1","prcp95th_lag2","prcp95th_lag3","prcp95th_lag4","prcp95th_lag5","prcp95th_lag6", "prcp95th_lag7")

#Now define the regions and separate the FIPs in the general data frame into lists
regions <- as.character(unique(mydata$FIPS))
dlist <- lapply(regions,function(x) mydata[mydata$FIPS==x,])
names(dlist) <- regions

#Loop through the different mortalities
mod_results <- data.frame()
for(death_type in subgroups){
  #Define a data frame to hold the model results
  metaresults <- data.frame(cat=lags, fit = NA, se = NA)
  
  #For loop to loop through the FIPs
  for(i in seq(length(dlist))){
    #Print the FIPs as we run through the for loop
    cat(i, "")
    
    #Extract the data
    subdata <- dlist[[i]]
    
    #Now define the for loop through the lag days for the actual model
    for(j in 0:7){
      #To get around the algorithm not converging for some counties
      if(death_type == "totdeath"){
        mod1 <- glm(subdata[, death_type] ~ as.factor(subdata[, paste0("prcp95th_lag", j)]) 
                    + ns(time, 4*6) + ns(subdata[, paste0("templag", j)], 5) + dow,
                    data = subdata, family = quasipoisson)
      }else{
        trynext = try(
        mod1 <- glm(subdata[, death_type] ~ as.factor(subdata[, paste0("prcp95th_lag", j)]) 
                    + ns(time, 4*6) + ns(subdata[, paste0("templag", j)], 5) + dow,
                    data = subdata, family = quasipoisson), silent = F)
        #To get around the algorithm not converging for some counties
        if("try-error" %in% class(trynext)){
          next
        }
      
        mod1 <- glm(subdata[, death_type] ~ as.factor(subdata[, paste0("prcp95th_lag", j)]) 
                      + ns(time, 4*6) + ns(subdata[, paste0("templag", j)], 5) + dow,
                      data = subdata, family = quasipoisson)
      }
      metaresults[j+1, 2:3] <- c(summary(mod1)$coef[2,1], summary(mod1)$coef[2,2])
      metaresults$dis[j+1] <- death_type
      metaresults$county[j+1] <- unique(subdata$FIPS)
    }
    #Create a data frame to hold the results
    if(i == 1){
      results <- metaresults
    }else{
      results <- rbind(results, metaresults)
    }
  }
  mod_results <- rbind(mod_results, results)
}

output_results <- lapply(subgroups, function(x) mod_results[mod_results$dis == x, ])

temp <- output_results

#Now we conduct the meta-analysis
#Get rid of the NAs and redefine the lags, vars, along with a new metaresults dataframe
countyresult<-na.omit(temp)
lags <-  c("prcp95th_lag0","prcp95th_lag1","prcp95th_lag2","prcp95th_lag3","prcp95th_lag4","prcp95th_lag5","prcp95th_lag6", "prcp95th_lag7")


for(i in 1:length(subgroups)){
  #First subset the lists to the correct death type
  cat(subgroups[i], "")
  data <- countyresult[[i]]
  
  #Now define a data frame to hold the model results for each lag type
  metaresults <- data.frame(cat=lags, fit = NA, low = NA, upper = NA)
  
  #Now we can run the meta-analysis model
  for(j in seq(length(lags))){
    #Print the lag we are on
    cat(j, "")
    #Subset the data further on the lag type
    prcp_lag <- subset(data, data$cat == lags[j])
    
    fit_est <- prcp_lag$fit
    fit_se <- prcp_lag$se
    
    #Now for the meta-analysis model
    metamod <- rma(yi = fit_est, sei = fit_se, data = prcp_lag, method = "REML")
    metaresults[j, 2] <- predict(metamod, transf = exp, digits = 4)$pred
    metaresults[j, 3] <- predict(metamod, transf = exp, digits = 4)$ci.lb
    metaresults[j, 4] <- predict(metamod, transf = exp, digits = 4)$ci.ub
    
    metaresults$dis[1:nrow(metaresults)] <- subgroups[i]
    metaresults$thres[1:nrow(metaresults)] <- "95th"
  }
  
  #Now we create a place to store the results for each mortality type
  if(i == 1){
    allmetaresult <- metaresults
  }else{
    allmetaresult <- rbind(allmetaresult, metaresults)
  }
}

holder <- allmetaresult
holder$Analysis <- "Subgroup"
```

We opted to define our base model as the basis since the use of the 95th percentile is common in environmental epidemiology literature linking exposure and health outcomes. Another motivator was when we saw a lack of significance in our 90th percentile definition. Across the board, it seems like our model remained robust against the sensitivity analyses and preserved similar lag structure and significance. Please refer to the appendix for a table with the full sensitivity analysis results. We also note that aside from the shift in percentiles, the interpretability of our model doesn’t change significantly since we can always obtain the relative risk by cancelling out the logarithm. We then performed a subgroup analysis for total mortality and focused on the results for lag day 3 since it was the lag that provided us with the most significance for total mortality in the basis model.

Results for sex displayed a significant association in males (RR = 1.0331, 95% CI: 1.0112, 1.0554), but not females (RR = 1.0150, 95% CI: 0.9926, 1.0379). For age, we found a significant association in the populations of ages 0-64 (RR = 1.0330, 95% CI: 1.0031, 1.0639) and 75+ (RR = 1.0271, 95% CI: 1.0051, 1.0496). The population of ages 65-74 was not significantly associated (RR = 1.0175, 95% CI: 0.9832, 1.0531). The non-Hispanic black subgroup was found to not be statistically significant (RR = 1.0148, 95% CI: 0.9817, 1.0490) while the non-Hispanic white subgroup was found to have a significant association (RR = 1.0225, 95% CI: 1.0061, 1.0390). We did not find significant differences amongst our subgroup results. There is also no significant association in the warm season defined to be May to September (RR = 1.0105, 95% CI: 0.9899, 1.0316), as opposed to a significant association in the cold season defined to be October to April (RR = 1.0318, 95% CI: 1.0129, 1.0510). In our study, compared to non-extreme precipitation days, extreme precipitation exposure was found to be associated with a 2.24%, 2.37%, and 6.92% increased risk in total, non-accidental, and external mortality respectively. The uptick occurs 3 days after initial exposure for total and non-accidental mortality, and 1 day after initial exposure for external mortality. A previous study on the relationship between increased precipitation and mortality in northern Ghana found that a 10 mm increase in precipitation at lag days 2-6 was significantly associated with a 1.71% increase in total mortality, which supports our study’s outcome of a positive relationship between high rainfall exposure and mortality despite the two projects differing in exposure metrics. 

Extreme rainfall events could induce mortality risk in cardiovascular and respiratory diseases in addition to the effect on communicable diseases highlighted in previous studies. We found that short-term exposure to an extreme rainfall event was significantly associated with a 3.58% increase in cardiovascular disease mortality and a 6.67% increase in respiratory disease mortality on lag days 0 and 3 respectively. As reported by previous studies on weather disasters related to precipitation, hurricanes and floods could increase the mortality risks of these two non-communicable diseases by 3.7% and 34% respectively. Additional support for our results can be found in papers analyzing the association between extreme rainfall exposure and morbidity. Tang et al.’s 2020 study found, with the same classification using the 95th percentile, that extreme rainfall is significantly associated with ischemic stroke hospitalizations with a single-day effect occurring on lag day 3 with RR = 1.040 (95% CI: 1.058-1.073) in Hefei, China. In our results, we also found that the associated increase in cardiovascular mortality could last for three days after the initial exposure. Smith et al.’s 2017 study on extreme precipitation and emergency room (ER) visits for influenza in Massachusetts found that exposure was associated with a relative risk of 1.23 (95% CI: 1.16, 1.30) for influenza ER visits at lag days 0-6. Despite our differing methodologies, study populations, and relative risks, the studies above support our findings on cardiovascular and respiratory outcomes. 

Our subgroup analysis with two-sample z-tests did not find any evidence to support a specific subgroup being more vulnerable to extreme rainfall events than the other groups within the same category. While we did find that the male, 0-64, 75+, non-Hispanic White, and cold season subgroups had significant mortality risks associated with extreme rainfall events, the two-sample z-tests did not produce any significant results (see Table 2). There is, however, existing literature studying extreme rainfall exposure on health outcomes that identify similar subgroups to the ones above as groups at risk of higher vulnerability. Tang et al.’s 2020 study found that males were more susceptible than females (p-value = 0.008 vs. 0.414)16. Smith et al.’s 2017 study on extreme precipitation and emergency room visits for influenza found significant associations in children ages 5-18 years old and adults ages 19-64, matching our 0-64 age group significance. In Han et al.’s 2017 study about estimating the national burden of disease associated with heavy precipitation and typhoons in Korea, they found that males and those aged 65+ were also more vulnerable.

There are several possible mechanisms that can explain our observed associations. First, extreme rainfall events leave behind extensive damages that have negative impacts several days after exposure. Heavy rainfall can lead to floods that drown civilians or down power lines to electrocute them. Damage from extreme rainfall can be conducive to infectious and parasitic diseases due to the still water left behind along with the compromised drinking water and sanitation. Second, respiratory disease deaths can be related to disrupted power supplies for breathing aids. Additionally, rainwater has been shown to cause osmotic shock which ruptures grass pollens that could then contribute to a release of aeroallergens with the potential to exacerbate asthma. Exposure to precipitation could be associated with airway inflammation for longer moving averages because mold or fungus may be a plausible mediator as concentrations peak after 3-7 days. Both mechanisms could explain the findings from Soneja et al.’s 2016 study, which found that summertime extreme precipitation events increase the risk of hospitalization for asthma by 11% in Maryland. Third, cardiovascular disease deaths can be linked to heart attacks and cardiac arrests from stress, overexertion, disruption of treatment, and even abnormal response of cardiopulmonary brought by extreme precipitation disaster. Zhang et al.’s 2022 study on the association between asthma and all-cause mortality showed that asthma patients have a 25% increased risk of cardiovascular mortality compared to people without asthma where RR = 1.25 (95% CI: 1.14-1.38).

## Conclusions and recommendations
Our study suggests a positive association between extreme rainfall exposure and cause-specific mortality over the course of four years in North Carolina. Specifically, we employed a quasi-poisson generalized linear model to quantify extreme rainfall exposure’s influence on cause-specific mortality with a numerical percentage and a specific time frame. We further analyzed age, sex, and race subgroups within the population and while we did not find any significant differences within the groups, we still saw general significance with many of the confidence intervals being above 1.00. Our study contributed to the knowledge gap between extreme rainfall and mortality and our research could provide a basis for estimating the related death burden from past events to projections of future events. Doing so would further illuminate the temporal-spatial transition trends of health risks associated with extreme rainfall events under climate change. Our study was also by no means perfect. With any exposure-response relationship, there is the possibility of exposure-misclassification since we evaluated on the county level. Our assumption here is that the entire county’s population receives a uniform level of exposure, which is not necessarily true as residents commute regularly. Published literature, however, suggests that exposure misclassification would only bias our results towards the null. Another opportunity for improvement would be distinguishing between different rainfall magnitudes within the top 5% of extreme rainfall. There are naturally instances in our data where the measurement barely exceeds the 95th percentile threshold, but other instances where the threshold is met due to a coastal hurricane. One last limitation would be the low cases of mortality in a few North Carolina counties. The low number of samples leads to models for that county becoming unable to converge and as a result, led to us excluding those three counties from consideration. The trade-off here though is that we lose statistical power in our results, but it is also worth recognizing that we already have a large sample size for our study as the data tracks 100 counties over a time period of four years.



## Appendix (optional)
The appendix was appended to the actual report
